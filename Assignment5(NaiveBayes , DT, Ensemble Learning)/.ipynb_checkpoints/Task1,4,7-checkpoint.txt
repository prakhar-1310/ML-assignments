Task 1: Theory Questions Answer in 2–4 sentences:

Q1. What is the core assumption of Naive Bayes?
Ans. The core assumption of Naive Bayes is that all features are conditionally independent of each other given the class label. This means the presence (or value) of one feature does not affect the presence (or value) of another, simplifying computation.


Q2. Differentiate between GaussianNB, MultinomialNB, and BernoulliNB.
Ans. -->GaussianNB assumes that features follow a normal (Gaussian) distribution and is suitable for continuous data.
-->MultinomialNB is used for discrete count data (e.g., word counts in text classification).
-->BernoulliNB is ideal for binary/boolean features, where each feature is either 0 or 1 (e.g., word presence/absence).


Q3. Why is Naive Bayes considered suitable for high-dimensional data?
Ans. Naive Bayes is efficient in high-dimensional spaces because it doesn’t require feature selection or complex computations; its independence assumption allows it to scale linearly with the number of features and perform well even with thousands of dimensions, as often seen in text classification tasks.




Task 4: Conceptual Questions Answer briefly:

Q1. What is entropy and information gain?
Ans. -->Entropy is a measure of impurity or randomness in a dataset. It tells us how mixed the class labels are.
If all the data points belong to the same class, entropy is 0 (pure). Higher entropy means more disorder
and more mixed classes.
-->Information Gain is the amount of reduction in entropy when a dataset is split based on a feature.
It helps identify the best feature for splitting the data. A higher information gain means the feature
gives more useful information for classifying the data.


Q2. Explain the difference between Gini Index and Entropy.
Ans. Both Gini Index and Entropy measure the impurity of a dataset, but they do it differently.
The Gini Index calculates how often a randomly chosen element would be incorrectly classified.
It’s simpler and faster to compute, as it doesn’t use logarithms.
Entropy is based on information theory and measures the level of disorder or uncertainty. It uses 
logarithmic calculations.
In practice, both give similar results, but Gini is usually preferred for speed, while entropy
is preferred when using algorithms like ID3 or C4.5.


Q3. How can a decision tree overfit? How can this be avoided?
Ans. A decision tree overfits when it becomes too complex and learns not only the patterns in the 
training data but also the noise or outliers. This leads to poor performance on new, unseen data.

To avoid overfitting:
-->Use pruning, which removes unnecessary branches from the tree.
-->Set a maximum depth for the tree to prevent it from growing too large.
-->Set a minimum number of samples required to split a node or to be in a leaf.
-->use ensemble methods like Random Forest, which combine multiple decision trees to reduce overfitting.




Task 7: Conceptual Questions Answer:

1. What is the difference between Bagging and Boosting?
Ans. Bagging (Bootstrap Aggregating):-
Builds multiple models independently using random subsets of the training data (with replacement).
Averages their predictions (or uses majority voting) to improve stability and reduce variance.
Examples: Random Forest, Bagged Decision Trees.
Boosting:-
Builds models sequentially, where each new model focuses on correcting errors made by the previous ones.
Combines them to form a strong learner.
Reduces both bias and variance, but more prone to overfitting.
Examples: AdaBoost, Gradient Boosting, XGBoost.

2. How does Random Forest reduce variance?
Ans. Random Forest builds multiple decision trees using different random subsets of:-
Training data (via bootstrapping).
Features (random feature selection at each split).
By averaging predictions from many uncorrelated trees, it:
Smooths out noise from individual trees.
Reduces overfitting.
Results in lower variance and more robust predictions compared to a single decision tree.

3. What is the weakness of boosting-based methods?
Ans. Sensitive to Noisy Data and Outliers:-
Because boosting tries to correct the errors of previous models, it can over-focus on noisy or mislabelled data, leading to overfitting.
Longer Training Time:-
Boosting is sequential, so it can be slower to train compared to parallel methods like bagging.
Complexity:
Boosting models are more complex and harder to interpret than simpler models like decision trees.

